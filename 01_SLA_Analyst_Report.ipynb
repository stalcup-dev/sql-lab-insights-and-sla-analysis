{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# üß™ Clinical Lab SLA Analysis Report\n",
    "**Author:** Allen Stalcup  \n",
    "**Role:** Medical Laboratory Technician ‚Üí Data Analyst Pivot  \n",
    "**Tools:** PostgreSQL, Python (psycopg2, pandas, matplotlib), Jupyter  \n",
    "\n",
    "---\n",
    "### **Objective**\n",
    "Analyze sample laboratory turnaround performance (TAT) and SLA compliance using SQL-based analytics.\n",
    "The project simulates a reference lab environment where specimens arrive throughout the day,  \n",
    "and performance is monitored by **shift**, **site**, and **analyte type**.\n",
    "\n",
    "---\n",
    "### **Key Questions**\n",
    "1. How does SLA compliance vary by **shift** (Day, Evening, Night)?\n",
    "2. Which **sites** consistently meet SLA targets?\n",
    "3. What analytes show **high or low turnaround percentiles** relative to SLA?\n",
    "4. How can visualization highlight throughput bottlenecks?\n",
    "\n",
    "---\n",
    "### **Tech Stack**\n",
    "- **Database:** PostgreSQL (synthetic dataset under `synth` schema)\n",
    "- **Queries:** Modular `.sql` files\n",
    "- **Notebook:** Python Jupyter for visualization + storytelling\n",
    "- **Visualization:** Matplotlib (no hard-coded colors to ensure portability)\n",
    "\n",
    "## üß© Data Model Overview\n",
    "The database includes 3 key tables:\n",
    "\n",
    "| Table | Description |\n",
    "|-------|--------------|\n",
    "| `synth.specimens` | Tracks received and collected timestamps per sample |\n",
    "| `synth.results` | Contains verified test results linked to specimens |\n",
    "| `synth.analytes` | Defines SLA targets and test metadata |\n",
    "\n",
    "Each result joins a specimen (via `specimen_id`) and an analyte (via `analyte_code`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Environment + installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in a fresh environment, uncomment:\n",
    "# %pip install psycopg2-binary pandas matplotlib python-dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "load_dotenv()  # loads .env if present\n",
    "\n",
    "# Folders\n",
    "OUT = Path(\"../artifacts\"); OUT.mkdir(parents=True, exist_ok=True)\n",
    "FIG = OUT / \"figs\"; FIG.mkdir(parents=True, exist_ok=True)\n",
    "CSV = OUT / \"csv\";  CSV.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file (kept local / private)\n",
    "load_dotenv()\n",
    "\n",
    "# Reusable database query function\n",
    "def run_query(sql: str) -> pd.DataFrame:\n",
    "    conn = psycopg2.connect(\n",
    "        host=os.getenv(\"PG_HOST\"),\n",
    "        port=os.getenv(\"PG_PORT\"),\n",
    "        dbname=os.getenv(\"PG_DB\"),\n",
    "        user=os.getenv(\"PG_USER\"),\n",
    "        password=os.getenv(\"PG_PASSWORD\")\n",
    "    )\n",
    "    df = pd.read_sql(sql, conn)\n",
    "    conn.close()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### DB connection + helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path=\"path/to/.env\")\n",
    "\n",
    "\n",
    "print(\"PG_HOST:\", os.getenv(\"PG_HOST\"))\n",
    "print(\"PG_DB:\", os.getenv(\"PG_DB\"))\n",
    "print(\"PG_USER:\", os.getenv(\"PG_USER\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# --- Load environment variables ---\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # loads from current working directory\n",
    "\n",
    "\n",
    "# --- Retrieve credentials ---\n",
    "PG_HOST = os.getenv(\"PG_HOST\")\n",
    "PG_PORT = os.getenv(\"PG_PORT\")\n",
    "PG_DB = os.getenv(\"PG_DB\")\n",
    "PG_USER = os.getenv(\"PG_USER\")\n",
    "PG_PASSWORD = os.getenv(\"PG_PASSWORD\")\n",
    "\n",
    "# --- Connection function ---\n",
    "def run_sql(sql: str) -> pd.DataFrame:\n",
    "    with psycopg2.connect(\n",
    "        host=PG_HOST,\n",
    "        port=PG_PORT,\n",
    "        dbname=PG_DB,\n",
    "        user=PG_USER,\n",
    "        password=PG_PASSWORD\n",
    "    ) as conn:\n",
    "        return pd.read_sql(sql, conn)\n",
    "\n",
    "print(f\"Connected to {PG_DB} on {PG_HOST}:{PG_PORT} as {PG_USER}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... top of file (after imports)\n",
    "from pathlib import Path\n",
    "\n",
    "# existing OUT dir not required for images anymore; keep if you want CSVs there\n",
    "VIS = Path(\"./visuals\")\n",
    "VIS.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Canonical SLA views (quick pull)\n",
    "\n",
    "**What:** Load standardized SLA metrics from SQL views so we slice consistently.\n",
    "- `synth.sla_shift_v` ‚Üí SLA by **shift** (Day/Evening/Night)\n",
    "- `synth.sla_site_v`  ‚Üí SLA by **site** (ClinicA/B/C)\n",
    "\n",
    "**How SLA is calculated (once, in SQL):**\n",
    "\n",
    "SLA % = 100 * AVG((verified_ts - received_ts <= tat_target_minutes)::int)\n",
    "Also returns `avg_tat_min` and `n` (row count).\n",
    "\n",
    "**Read the table:**\n",
    "- Higher `sla_hit_pct` = better compliance\n",
    "- Use `n` to judge stability (tiny n can be noisy)\n",
    "\n",
    "**Quick take from this pull:** Night ‚âà best, Evening ‚âà solid, Day ‚âà lowest (peak intake window).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shift = run_sql(\"SELECT * FROM synth.sla_shift_v;\")\n",
    "df_site  = run_sql(\"SELECT * FROM synth.sla_site_v;\")\n",
    "\n",
    "# Optional stricter order-level metric: uncomment if you created it\n",
    "# df_order_shift = run_sql(\"SELECT * FROM synth.sla_order_shift_v;\")\n",
    "\n",
    "df_shift, df_site.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### QC Impact & Rolling Intake (Operational Pulse)\n",
    "\n",
    "**Goal:**  \n",
    "Measure how QC (quality control) failures and specimen intake volume affect turnaround performance.\n",
    "\n",
    "- `SQL_QC_IMPACT` ‚Üí Compares **average TAT** for results near a QC **fail** vs. normal operation.  \n",
    "  - `near_fail = True` ‚Üí Bench had a QC fail within 60 minutes of result verification.\n",
    "  - Highlights how instability impacts workflow speed.\n",
    "\n",
    "- `SQL_ROLLING` ‚Üí Tracks **hourly intake trends** with a **6-hour rolling total**.  \n",
    "  - Useful for visualizing throughput surges (e.g., afternoon specimen spikes).\n",
    "\n",
    "**Output:**  \n",
    "- `df_qc` ‚Üí Average TAT + sample counts grouped by QC condition.  \n",
    "- `df_roll` ‚Üí Hourly received counts and rolling workload totals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_QC_IMPACT = \"\"\"\n",
    "WITH j AS (\n",
    "  SELECT a.bench,\n",
    "         EXTRACT(EPOCH FROM (r.verified_ts - s.received_ts))/60 AS tat_lab_min,\n",
    "         EXISTS (\n",
    "           SELECT 1\n",
    "           FROM synth.qc_events q\n",
    "           WHERE q.bench = a.bench\n",
    "             AND q.severity = 'fail'\n",
    "             AND q.event_ts BETWEEN r.verified_ts - INTERVAL '60 minutes' AND r.verified_ts\n",
    "         ) AS near_fail\n",
    "  FROM synth.results r\n",
    "  JOIN synth.specimens s USING (specimen_id)\n",
    "  JOIN synth.analytes  a USING (analyte_code)\n",
    ")\n",
    "SELECT near_fail, ROUND(AVG(tat_lab_min),1) AS avg_tat, COUNT(*) AS n\n",
    "FROM j\n",
    "GROUP BY near_fail\n",
    "ORDER BY near_fail;\n",
    "\"\"\"\n",
    "\n",
    "SQL_ROLLING = \"\"\"\n",
    "WITH timeline AS (\n",
    "  SELECT generate_series(\n",
    "           date_trunc('hour', MIN(received_ts)),\n",
    "           date_trunc('hour', MAX(received_ts)),\n",
    "           interval '1 hour'\n",
    "         ) AS hr\n",
    "  FROM synth.specimens\n",
    "),\n",
    "counts AS (\n",
    "  SELECT t.hr, COUNT(*) AS received_count\n",
    "  FROM timeline t\n",
    "  JOIN synth.specimens s\n",
    "    ON s.received_ts >= t.hr AND s.received_ts < t.hr + interval '1 hour'\n",
    "  GROUP BY t.hr\n",
    ")\n",
    "SELECT hr,\n",
    "       received_count,\n",
    "       SUM(received_count) OVER (ORDER BY hr ROWS BETWEEN 5 PRECEDING AND CURRENT ROW) AS rolling_6hr_total\n",
    "FROM counts\n",
    "ORDER BY hr;\n",
    "\"\"\"\n",
    "\n",
    "df_qc   = run_sql(SQL_QC_IMPACT)\n",
    "df_roll = run_sql(SQL_ROLLING)\n",
    "\n",
    "df_qc, df_roll.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "\n",
    "**Takeaway:**  \n",
    "QC disruptions double lab TAT (‚âà34 ‚Üí 73 min). Rolling intake quantifies the load curve driving these variances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Save tidy CSVs (portfolio-friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shift.to_csv(CSV / \"sla_by_shift.csv\", index=False)\n",
    "df_site.to_csv(CSV / \"sla_by_site.csv\", index=False)\n",
    "df_qc.to_csv(CSV / \"qc_fail_impact.csv\", index=False)\n",
    "df_roll.to_csv(CSV / \"rolling_6hr.csv\", index=False)\n",
    "CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### SLA by Shift (Result-Level)\n",
    "\n",
    "**Goal:**  \n",
    "Visualize turnaround performance across operational shifts to identify timing-related efficiency gaps.\n",
    "\n",
    "**Query:**  \n",
    "`synth.sla_shift_v` ‚Äî aggregates result-level SLA compliance by **Day / Evening / Night** shift.\n",
    "\n",
    "**Output:** \n",
    "\n",
    "Each bar represents the percentage of results verified within their SLA target (‚â§ tat_target_minutes).\n",
    "\n",
    "**Insight:**  \n",
    "- **Night shift:** 100% compliance ‚Äî optimal performance with minimal load.  \n",
    "- **Evening shift:** Slight dip (‚âà99%) ‚Äî consistent but less efficient than Night.  \n",
    "- **Day shift:** Noticeable drop (‚âà92%) ‚Äî indicates workload congestion or resource strain during peak hours.\n",
    "\n",
    "**Takeaway:**  \n",
    "Day operations face the heaviest volume and longest turnaround variance. Scheduling or staffing calibration could lift SLA compliance closer to 98‚Äì100%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "plt.bar(df_shift[\"shift\"], df_shift[\"sla_hit_pct\"])\n",
    "plt.title(\"SLA Compliance by Shift (Result-Level)\")\n",
    "plt.xlabel(\"Shift\")\n",
    "plt.ylabel(\"SLA Compliance (%)\")\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(df_shift[\"sla_hit_pct\"]):\n",
    "    plt.text(i, v + 1, f\"{v:.1f}%\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show in notebook\n",
    "\n",
    "# ... your plotting code (plt.bar, labels, etc.)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save FIRST, then show\n",
    "plt.savefig(VIS / \"fig_sla_by_shift.png\", dpi=300,\n",
    "            bbox_inches=\"tight\", facecolor=\"white\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Chart: SLA by site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.barh(df_site[\"source_site\"], df_site[\"sla_hit_pct\"])\n",
    "plt.title(\"SLA Compliance by Site (Result-Level)\")\n",
    "plt.xlabel(\"SLA Compliance (%)\")\n",
    "plt.ylabel(\"Source Site\")\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(df_site[\"sla_hit_pct\"]):\n",
    "    plt.text(v + 1, i, f\"{v:.1f}%\", va=\"center\", fontsize=9)\n",
    "\n",
    "plt.xlim(0, 100)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# ... your plotting code (plt.bar, labels, etc.)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save FIRST, then show\n",
    "plt.savefig(VIS / \"fig_sla_by_site.png\", dpi=300,\n",
    "            bbox_inches=\"tight\", facecolor=\"white\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "**Takeaway:**  \n",
    "The ED likely faces heavier specimen volume or more complex workflows during peak hours.  \n",
    "Prioritizing **resource balancing, staffing, or process automation** in the ED could restore overall SLA compliance to near-perfect levels.\n",
    "\n",
    "Investigating wether the ED Lab intakes specimens from outside sources required. (Check for Cross-Site Transfers)\n",
    "\n",
    "*If the collection-to-receipt delay > 60 minutes, it‚Äôs probably not collected on-site ‚Äî i.e., transported from an external facility.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Chart: QC fail impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Normal\", \"QC Fail\"]\n",
    "vals = []\n",
    "for flag in [False, True]:\n",
    "    row = df_qc[df_qc[\"near_fail\"] == flag]\n",
    "    vals.append(float(row[\"avg_tat\"].iloc[0]) if not row.empty else 0.0)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(labels, vals)\n",
    "plt.title(\"QC Fail Proximity Impact on Lab TAT\")\n",
    "plt.ylabel(\"Average Lab TAT (minutes)\")\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(vals):\n",
    "    plt.text(i, v + 1, f\"{v:.1f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Save FIRST, then show\n",
    "plt.savefig(VIS / \"fig_qc_fail_impact.png\", dpi=300,\n",
    "            bbox_inches=\"tight\", facecolor=\"white\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "**Takeaway:**  \n",
    "QC incidents introduce measurable slowdowns ‚Äî likely due to instrument recalibration, result verification delays, or reruns.  \n",
    "Reducing QC event frequency or improving post-failure recovery could yield **>50% faster throughput** during critical periods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Chart: Rolling 6-hour intake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(df_roll[\"hr\"], df_roll[\"rolling_6hr_total\"], linewidth=2)\n",
    "plt.title(\"Rolling 6-Hour Specimen Intake Volume\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Specimens Received (6hr rolling)\")\n",
    "\n",
    "# Format timestamps on X-axis neatly\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Save FIRST, then show\n",
    "plt.savefig(VIS / \"fig_rolling_6hr.png\", dpi=300,\n",
    "            bbox_inches=\"tight\", facecolor=\"white\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "**Takeaway:**  \n",
    "Operationally, this confirms a **stable daily rhythm** but highlights opportunities to **redistribute staffing or automation** during intake spikes to maintain SLA compliance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "**Goal** ‚Äî Percentile-by-Analyte TAT vs. SLA\n",
    "\n",
    "Compute P50/P90/P95 turnaround times per analyte (verified_ts ‚àí received_ts in minutes) and compare to each analyte‚Äôs SLA target. This reveals median performance, tail risk (P90/P95), and where tests are most likely to breach SLA, so we can prioritize workflow or capacity fixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_PCT = \"\"\"\n",
    "WITH m AS (\n",
    "  SELECT a.analyte_code,\n",
    "         EXTRACT(EPOCH FROM (r.verified_ts - s.received_ts))/60 AS tat_min,\n",
    "         a.tat_target_minutes AS sla_min\n",
    "  FROM synth.results r\n",
    "  JOIN synth.specimens s USING (specimen_id)\n",
    "  JOIN synth.analytes  a USING (analyte_code)\n",
    ")\n",
    "SELECT analyte_code,\n",
    "       ROUND(PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY tat_min)::numeric,1) AS p50,\n",
    "       ROUND(PERCENTILE_CONT(0.9) WITHIN GROUP (ORDER BY tat_min)::numeric,1) AS p90,\n",
    "       ROUND(PERCENTILE_CONT(0.95)WITHIN GROUP (ORDER BY tat_min)::numeric,1) AS p95,\n",
    "       MAX(sla_min) AS sla_min\n",
    "FROM m\n",
    "GROUP BY analyte_code\n",
    "ORDER BY analyte_code;\n",
    "\"\"\"\n",
    "df_pct = run_sql(SQL_PCT)\n",
    "df_pct.to_csv(CSV / \"percentiles_by_analyte.csv\", index=False)\n",
    "df_pct\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "# Plot bars for percentiles\n",
    "plt.bar(df_pct[\"analyte_code\"], df_pct[\"p95\"], alpha=0.6, label=\"P95\")\n",
    "plt.bar(df_pct[\"analyte_code\"], df_pct[\"p90\"], alpha=0.6, label=\"P90\")\n",
    "plt.bar(df_pct[\"analyte_code\"], df_pct[\"p50\"], alpha=0.6, label=\"P50\")\n",
    "\n",
    "# Overlay SLA target line\n",
    "plt.plot(df_pct[\"analyte_code\"], df_pct[\"sla_min\"], color=\"red\", linewidth=2, label=\"SLA Target\")\n",
    "\n",
    "plt.title(\"Turnaround Time Percentiles by Analyte\")\n",
    "plt.xlabel(\"Analyte\")\n",
    "plt.ylabel(\"Minutes\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# ... your plotting code (plt.bar, labels, etc.)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save FIRST, then show\n",
    "plt.savefig(VIS / \"tat_percentiles_by_analyte.png\", dpi=300,\n",
    "            bbox_inches=\"tight\", facecolor=\"white\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Takeaway:\n",
    "Turnaround is generally strong, but CMP and LIPID panels could benefit from workflow refinement or instrument load balancing to tighten their upper-percentile completion times.\n",
    "\n",
    "A1C times likely affected by CBCs running before A1Cs on the same specimen.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "**SLA by Analyte √ó Shift**\n",
    "\n",
    "**Goal**\n",
    "See which tests (analytes) slip against SLA by time of day so you can target staffing, batching, or instrument scheduling.\n",
    "\n",
    "**What the table shows**\n",
    "Each row is an analyte; each shift (Day/Evening/Night) has:\n",
    "\n",
    "avg_tat_min ‚Äì average lab turnaround in minutes (receipt ‚Üí verification)\n",
    "\n",
    "sla_hit_pct ‚Äì % of results meeting that analyte‚Äôs SLA\n",
    "\n",
    "n ‚Äì result count (sample size / confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 ‚Äî Load and execute the SQL query\n",
    "sql_path = Path.cwd() / \"sql\" / \"sla_by_analyte_shift.sql\"\n",
    "\n",
    "# Safety check to prevent empty query errors\n",
    "SQL_SLA_ANALYTE_SHIFT = sql_path.read_text().strip()\n",
    "if not SQL_SLA_ANALYTE_SHIFT:\n",
    "    raise ValueError(f\"{sql_path} is empty ‚Äî recheck your SQL script\")\n",
    "\n",
    "# Run the query\n",
    "df_heat = run_query(SQL_SLA_ANALYTE_SHIFT)\n",
    "print(\"Rows returned:\", len(df_heat))\n",
    "df_heat.head()\n",
    "\n",
    "# Pivot to a matrix: analyte (rows) √ó shift (cols) with SLA %\n",
    "heat = (\n",
    "    df_heat\n",
    "      .pivot(index=\"analyte_code\", columns=\"shift\", values=\"sla_hit_pct\")\n",
    "      .reindex(columns=[\"Day\", \"Evening\", \"Night\"])                # consistent column order\n",
    ")\n",
    "\n",
    "# Optional: sort rows by average SLA (best ‚Üí worst)\n",
    "heat = heat.loc[heat.mean(axis=1).sort_values(ascending=False).index]\n",
    "\n",
    "# Make sure visuals folder exists\n",
    "from pathlib import Path\n",
    "VIS = Path(\"visuals\"); VIS.mkdir(exist_ok=True)\n",
    "\n",
    "# Plot heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(7, 4.5))\n",
    "im = plt.imshow(heat.values, aspect=\"auto\", vmin=85, vmax=100)      # bounded to highlight 85‚Äì100%\n",
    "plt.colorbar(im, label=\"SLA compliance (%)\")\n",
    "\n",
    "# Axes ticks/labels\n",
    "plt.xticks(range(heat.shape[1]), heat.columns)\n",
    "plt.yticks(range(heat.shape[0]), heat.index)\n",
    "plt.title(\"SLA Compliance by Analyte √ó Shift\")\n",
    "\n",
    "# Value annotations\n",
    "for i in range(heat.shape[0]):\n",
    "    for j in range(heat.shape[1]):\n",
    "        val = heat.iloc[i, j]\n",
    "        if pd.notna(val):\n",
    "            plt.text(j, i, f\"{val:.1f}%\", ha=\"center\", va=\"center\", fontsize=9)\n",
    "\n",
    "# ... your plotting code (plt.bar, labels, etc.)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save FIRST, then show\n",
    "plt.savefig(VIS / \"fig_sla_heatmap_analyte_shift.png\", dpi=300,\n",
    "            bbox_inches=\"tight\", facecolor=\"white\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "**Takeaway**\n",
    "Day shift is the bottleneck. CBC (81.2%) and PT/INR (88.8%) miss the 95‚Äì97% target; CMP (97.4%), A1C (96.3%), and LIPID (95.0%) are borderline but okay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Automated Textual Insights\n",
    "# ---------------------------\n",
    "\n",
    "def generate_sla_insights(df):\n",
    "    insights = []\n",
    "    overall_mean = df[\"sla_hit_pct\"].mean()\n",
    "\n",
    "    # Identify top and bottom performers\n",
    "    top = df.sort_values(\"sla_hit_pct\", ascending=False).head(3)\n",
    "    low = df.sort_values(\"sla_hit_pct\").head(3)\n",
    "\n",
    "    insights.append(f\"‚úÖ Overall average SLA compliance across analytes and shifts: **{overall_mean:.2f}%**.\\n\")\n",
    "\n",
    "    insights.append(\"üèÜ **Top-performing combinations:**\")\n",
    "    for _, row in top.iterrows():\n",
    "        insights.append(f\"- {row['analyte_code']} ({row['shift']}): {row['sla_hit_pct']:.2f}% SLA compliance.\")\n",
    "\n",
    "    insights.append(\"\\n‚ö†Ô∏è **Lowest-performing combinations:**\")\n",
    "    for _, row in low.iterrows():\n",
    "        diff = overall_mean - row['sla_hit_pct']\n",
    "        insights.append(f\"- {row['analyte_code']} ({row['shift']}): {row['sla_hit_pct']:.2f}% \"\n",
    "                        f\"(‚àí{diff:.1f}% below mean).\")\n",
    "\n",
    "    # Shift-level summary\n",
    "    shift_summary = (\n",
    "        df.groupby(\"shift\")[\"sla_hit_pct\"]\n",
    "        .mean()\n",
    "        .sort_values(ascending=False)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    insights.append(\"\\nüïí **Shift-level summary:**\")\n",
    "    for _, row in shift_summary.iterrows():\n",
    "        insights.append(f\"- {row['shift']}: {row['sla_hit_pct']:.2f}% average compliance.\")\n",
    "\n",
    "    return \"\\n\".join(insights)\n",
    "\n",
    "\n",
    "# Generate insights\n",
    "text_report = generate_sla_insights(df_heat)\n",
    "print(text_report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
